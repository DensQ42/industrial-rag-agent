{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d1b0a2",
   "metadata": {},
   "source": [
    "### Clear memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d1d5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec1645",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "755a9ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pymupdf, json, requests, re, sys\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import langchain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dee342e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e44d47bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "from src.data_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b0d17",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "Load chunks, prepared in the first notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f98862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text</th>\n",
       "      <th>page_num</th>\n",
       "      <th>char_count</th>\n",
       "      <th>start_char</th>\n",
       "      <th>end_char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>User Guide AWS Toolkit for Microsoft Azure Dev...</td>\n",
       "      <td>1</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AWS Toolkit for Microsoft Azure DevOps User Gu...</td>\n",
       "      <td>2</td>\n",
       "      <td>422</td>\n",
       "      <td>0</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>s likely to cause confusion among customers, o...</td>\n",
       "      <td>2</td>\n",
       "      <td>260</td>\n",
       "      <td>322</td>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_id                                               text  page_num  \\\n",
       "0         0  User Guide AWS Toolkit for Microsoft Azure Dev...         1   \n",
       "1         1  AWS Toolkit for Microsoft Azure DevOps User Gu...         2   \n",
       "2         2  s likely to cause confusion among customers, o...         2   \n",
       "\n",
       "   char_count  start_char  end_char  \n",
       "0         134           0       134  \n",
       "1         422           0       422  \n",
       "2         260         322       822  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = pd.read_json('../data/processed/chunks.json', orient='records')\n",
    "\n",
    "chunks.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2295b153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 569\n"
     ]
    }
   ],
   "source": [
    "print('Number of chunks:', chunks.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeade647",
   "metadata": {},
   "source": [
    "# Embeddings Creation\n",
    "Embeddings creation is different with LangChain. We need special wrapper that will be used by LangChain when it is needed (no need to explicitly manually create an embedding for each chunk). Here the same sentence transformer `all-MiniLM-L6-v2` model is used that comes within HuggingFace LangChain package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d312e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LangChain embedding function created\n"
     ]
    }
   ],
   "source": [
    "embedding_function = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "501b4962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Embedding dimension: 384\n",
      "ðŸ“Š First 5 values: [-0.04203960299491882, 0.04495583474636078, 0.016322309151291847, -0.03731227666139603, 0.03714952990412712]\n"
     ]
    }
   ],
   "source": [
    "# Test with a sample text\n",
    "test_text = \"AWS provides cloud computing services\"\n",
    "test_embedding = embedding_function.embed_query(test_text)\n",
    "\n",
    "print(f'ðŸ“Š Embedding dimension: {len(test_embedding)}')\n",
    "print(f'ðŸ“Š First 5 values: {test_embedding[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e72be39",
   "metadata": {},
   "source": [
    "### Create Embeddings for All Chunks (Optional Visualization)\n",
    "\n",
    "**Note:** We won't actually create and store embeddings manually here. LangChain will do it automatically in the next section when we create the vector store.\n",
    "\n",
    "But let's verify our embedding function works on all chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6153d0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing embedding function on 3 sample chunks...\n",
      "\n",
      "Chunk 1: 134 chars â†’ 384-dim embedding âœ“\n",
      "Chunk 2: 421 chars â†’ 384-dim embedding âœ“\n",
      "Chunk 3: 260 chars â†’ 384-dim embedding âœ“\n",
      "\n",
      "âœ… Embedding function works correctly!\n",
      "ðŸ“¦ Ready to embed all 569 chunks automatically in next section\n"
     ]
    }
   ],
   "source": [
    "# Test on first 3 chunks to verify\n",
    "sample_texts = chunks['text'].head(3).tolist()\n",
    "\n",
    "print('Testing embedding function on 3 sample chunks...\\n')\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    embedding = embedding_function.embed_query(text)\n",
    "    print(f'Chunk {i}: {len(text)} chars â†’ {len(embedding)}-dim embedding âœ“')\n",
    "\n",
    "print(f'\\nâœ… Embedding function works correctly!')\n",
    "print(f'ðŸ“¦ Ready to embed all {len(chunks)} chunks automatically in next section')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d1a83c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Difference from Notebook 2\n",
    "\n",
    "**Notebook 2:**\n",
    "```python\n",
    "# We did this:\n",
    "embeddings = model.encode(chunks['text'].tolist())  # Created all embeddings\n",
    "chunks['embedding'] = list(embeddings)  # Stored them\n",
    "```\n",
    "\n",
    "**Notebook 3:**\n",
    "```python\n",
    "# We just create the function:\n",
    "embedding_function = HuggingFaceEmbeddings(...)\n",
    "\n",
    "# LangChain will call embedding_function.embed_query(text) \n",
    "# automatically for each chunk when we create the vector store!\n",
    "```\n",
    "\n",
    "**Why this way?** \n",
    "- Less code to write\n",
    "- LangChain handles batching and optimization\n",
    "- Easier to swap embedding models later\n",
    "\n",
    "**Next:** We'll create the vector store, and LangChain will automatically embed all chunks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
