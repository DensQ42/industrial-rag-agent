{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56992a40",
   "metadata": {},
   "source": [
    "### Clear memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0530cb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1a1e51",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c093ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_precision, context_recall\n",
    "from datasets import Dataset\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2683d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c1e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "from src.langchain_RAG import setup_data_collection, langchain_rag_pipeline\n",
    "from src.data_utils import create_chunks, load_and_analyze_pdf, download_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca2f35",
   "metadata": {},
   "source": [
    "# Dataset Creation \n",
    "\n",
    "To evaluate different chunk sizes, overlap sizes, and $k$ values, we create a test dataset manually. The dataset consists of question-answer pairs. Each pair is labeled as either relevant or irrelevant to AWS documentation. There are 12 relevant and 3 irrelevant examples. also page numbers are provided for relevant answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f055c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = [\n",
    "    {\n",
    "        'id': 0,\n",
    "        'question': 'How do I make a tasty pizza?',\n",
    "        'reference_answer': 'This question is not related to AWS documentation.',\n",
    "        'page_numbers': [],\n",
    "        'category': 'irrelevant',\n",
    "    },\n",
    "    {\n",
    "        'id': 1,\n",
    "        'question': 'Who is the current president of USA?',\n",
    "        'reference_answer': 'This question is not related to AWS documentation.',\n",
    "        'page_numbers': [],\n",
    "        'category': 'irrelevant',\n",
    "    },\n",
    "    {\n",
    "        'id': 2,\n",
    "        'question': 'What is the capital of Germany?',\n",
    "        'reference_answer': 'This question is not related to AWS documentation.',\n",
    "        'page_numbers': [],\n",
    "        'category': 'irrelevant',\n",
    "    },\n",
    "    {\n",
    "        'id': 3,\n",
    "        'question': 'If I do not have an AWS account, what do I do?',\n",
    "        'reference_answer': 'To sign up for an AWS account open https://portal.aws.amazon.com/billing/signup and follow the online instructions.',\n",
    "        'page_numbers': [10],\n",
    "        'category': 'relevant',\n",
    "    },\n",
    "    {\n",
    "        'id': 4,\n",
    "        'question': 'What if I want to allow people outside of my AWS account to access my AWS resources?',\n",
    "        'reference_answer': 'You can create a role that users in other accounts or people outside of your organization can use to access your resources. You can specify who is trusted to assume the role. For services that support resource-based policies or access control lists (ACLs), you can use those policies to grant people access to your resources.',\n",
    "        'page_numbers': [114],\n",
    "        'category': 'relevant',\n",
    "    },\n",
    "    {\n",
    "        'id': 5,\n",
    "        'question': 'What is the AWS Toolkit for Microsoft Azure DevOps?',\n",
    "        'reference_answer': 'AWS Toolkit for Microsoft Azure DevOps is an extension for Microsoft Azure DevOps that contains tasks you can use in build and release definitions to interact with AWS services. It is available through the Visual Studio Marketplace.',\n",
    "        'page_numbers': [7],\n",
    "        'category': 'relevant',\n",
    "    },\n",
    "    {\n",
    "        'id': 6,\n",
    "        'question': 'Where can I install the AWS Toolkit for Azure DevOps extension?',\n",
    "        'reference_answer': 'You can install the AWS Toolkit for Azure DevOps extension from the Extensions for Azure DevOps Visual Studio Marketplace. The direct link is https://marketplace.visualstudio.com/items?itemName=AmazonWebServices.aws-vsts-tools',\n",
    "        'page_numbers': [12],\n",
    "        'category': 'relevant',\n",
    "    },\n",
    "    {\n",
    "        'id': 7,\n",
    "        'question': 'How many ways can I supply AWS credentials to tasks?',\n",
    "        'reference_answer': 'You can supply credentials in four ways: using a service connection, through named variables in your build (AWS.AccessKeyID, AWS.SecretAccessKey, AWS.SessionToken), through standard AWS environment variables in the build agent process, or with Amazon EC2 build agents using instance metadata.',\n",
    "        'page_numbers': [13, 14, 15],\n",
    "        'category': 'relevant',\n",
    "    },\n",
    "    {\n",
    "        'id': 8,\n",
    "        'question': 'What permissions does AWS Send SNS or SQS Message task require?',\n",
    "        'reference_answer': 'This task requires permissions to call the following AWS service APIs (depending on selected task options, not all APIs may be used): sns:GetTopicAttributes, sns:Publish, sqs:GetQueueAttributes and sqs:SendMessage.',\n",
    "        'page_numbers': [95],\n",
    "        'category': 'relevant',\n",
    "    },\n",
    "    {\n",
    "        'id': 9,\n",
    "        'question': 'Which deployment types does the AWS Lambda .NET Core task support?',\n",
    "        'reference_answer': 'The task supports two deployment types: Function (deploys a single function or creates a package zip file) and Serverless Application (performs deployment using AWS CloudFormation for multiple functions or builds and uploads to S3).',\n",
    "        'page_numbers': [76,77],\n",
    "        'category': 'relevant',\n",
    "    },\n",
    "    {\n",
    "        'id': 10,\n",
    "        'question': 'What structure does AWS CLI use?',\n",
    "        'reference_answer': 'The AWS CLI uses a multipart structure on the command line: <command> <subcommand> [options and parameters].',\n",
    "        'page_numbers': [33,34],\n",
    "        'category': 'relevant',\n",
    "    },\n",
    "    {\n",
    "        'id': 11,\n",
    "        'question': 'Can I create an S3 bucket automatically with the S3 Upload task?',\n",
    "        'reference_answer': 'Yes, you can select the checkbox \"Create S3 bucket if it does not exist\" and the task will attempt to create the bucket if it does not exist. Note that bucket names must be globally unique.',\n",
    "        'page_numbers': [79],\n",
    "        'category': 'relevant',\n",
    "    },\n",
    "    {\n",
    "        'id': 12,\n",
    "        'question': 'What is the maximum timeout value for the AWS CodeDeploy deployment task?',\n",
    "        'reference_answer': 'The default maximum timeout is 60 minutes.',\n",
    "        'page_numbers': [56],\n",
    "        'category': 'relevant',\n",
    "    },\n",
    "    {\n",
    "        'id': 13,\n",
    "        'question': 'What AWS Shell Script task is used for?',\n",
    "        'reference_answer': 'Runs a shell script in Bash, setting AWS credentials and Region information into the shell environment using the standard environment keys AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN and AWS_REGION.',\n",
    "        'page_numbers': [39],\n",
    "        'category': 'relevant',\n",
    "    },\n",
    "    {\n",
    "        'id': 14,\n",
    "        'question': 'Which task should I use to push a Docker image to Amazon ECR?',\n",
    "        'reference_answer': 'You should use the Amazon ECR Push task (also known as Amazon Elastic Container Registry Push Image Task). This task pushes a Docker image identified by name with optional tag, or image ID to ECR.',\n",
    "        'page_numbers': [51],\n",
    "        'category': 'relevant',\n",
    "    },\n",
    "]\n",
    "\n",
    "test_df = pd.DataFrame(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85e3aeb",
   "metadata": {},
   "source": [
    "Check dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f74b172e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 15 test questions\n",
      "Relevant: 12\n",
      "Irrelevant: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>page_numbers</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How do I make a tasty pizza?</td>\n",
       "      <td>This question is not related to AWS documentat...</td>\n",
       "      <td>[]</td>\n",
       "      <td>irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Who is the current president of USA?</td>\n",
       "      <td>This question is not related to AWS documentat...</td>\n",
       "      <td>[]</td>\n",
       "      <td>irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the capital of Germany?</td>\n",
       "      <td>This question is not related to AWS documentat...</td>\n",
       "      <td>[]</td>\n",
       "      <td>irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>If I do not have an AWS account, what do I do?</td>\n",
       "      <td>To sign up for an AWS account open https://por...</td>\n",
       "      <td>[10]</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>What if I want to allow people outside of my A...</td>\n",
       "      <td>You can create a role that users in other acco...</td>\n",
       "      <td>[114]</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           question  \\\n",
       "0   0                       How do I make a tasty pizza?   \n",
       "1   1               Who is the current president of USA?   \n",
       "2   2                    What is the capital of Germany?   \n",
       "3   3     If I do not have an AWS account, what do I do?   \n",
       "4   4  What if I want to allow people outside of my A...   \n",
       "\n",
       "                                    reference_answer page_numbers    category  \n",
       "0  This question is not related to AWS documentat...           []  irrelevant  \n",
       "1  This question is not related to AWS documentat...           []  irrelevant  \n",
       "2  This question is not related to AWS documentat...           []  irrelevant  \n",
       "3  To sign up for an AWS account open https://por...         [10]    relevant  \n",
       "4  You can create a role that users in other acco...        [114]    relevant  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Created {len(test_df)} test questions')\n",
    "print(f'Relevant: {(test_df[\"category\"] == \"relevant\").sum()}')\n",
    "print(f'Irrelevant: {(test_df[\"category\"] == \"irrelevant\").sum()}')\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb77cf55",
   "metadata": {},
   "source": [
    "# Prepare Data for RAGAS Evaluation \n",
    "\n",
    "### Database Setup\n",
    "Set up the vector store collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faa96106",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = setup_data_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4d1cc5",
   "metadata": {},
   "source": [
    "### Run RAG Pipeline on Test Dataset\n",
    "\n",
    "Run the LangChain RAG pipeline to generate an answer for each question in the test dataset. We collect all generated answers, along with the original questions, retrieved context chunks, and reference answers, into a DataFrame. We use Claude Haiku 4.5 as the LLM with temperature set to 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db744b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc='Running RAG'):\n",
    "    question = row['question']\n",
    "\n",
    "    rag_result = langchain_rag_pipeline(query=question, vectorstore=vectorstore, k=5)\n",
    "\n",
    "    eval_results.append({\n",
    "        'question': question,\n",
    "        'answer': rag_result['answer'],\n",
    "        'contexts': rag_result['contexts'],\n",
    "        'ground_truth': row['reference_answer'],\n",
    "    })\n",
    "\n",
    "\n",
    "eval_df = pd.DataFrame(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e794d",
   "metadata": {},
   "source": [
    "Save and load dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_pickle('../data/processed/eval_results.pkl')\n",
    "\n",
    "eval_df = pd.read_pickle('../data/processed/eval_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8f3367",
   "metadata": {},
   "source": [
    "Check dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34dbbf64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I make a tasty pizza?</td>\n",
       "      <td>The provided documentation chunks do not conta...</td>\n",
       "      <td>[ate Stack ......................................</td>\n",
       "      <td>This question is not related to AWS documentat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is the current president of USA?</td>\n",
       "      <td>This question is not answered in the provided ...</td>\n",
       "      <td>[WS account 1. Open https://portal.aws.amazon....</td>\n",
       "      <td>This question is not related to AWS documentat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the capital of Germany?</td>\n",
       "      <td>The answer to this question is not contained i...</td>\n",
       "      <td>[ble. Note: The Regions listed in the picker a...</td>\n",
       "      <td>This question is not related to AWS documentat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If I do not have an AWS account, what do I do?</td>\n",
       "      <td>Based on the provided documentation, if you do...</td>\n",
       "      <td>[WS account 1. Open https://portal.aws.amazon....</td>\n",
       "      <td>To sign up for an AWS account open https://por...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What if I want to allow people outside of my A...</td>\n",
       "      <td>According to the documentation, if you want to...</td>\n",
       "      <td>[perform: iam:PassRole In this case, Mary's po...</td>\n",
       "      <td>You can create a role that users in other acco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                       How do I make a tasty pizza?   \n",
       "1               Who is the current president of USA?   \n",
       "2                    What is the capital of Germany?   \n",
       "3     If I do not have an AWS account, what do I do?   \n",
       "4  What if I want to allow people outside of my A...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The provided documentation chunks do not conta...   \n",
       "1  This question is not answered in the provided ...   \n",
       "2  The answer to this question is not contained i...   \n",
       "3  Based on the provided documentation, if you do...   \n",
       "4  According to the documentation, if you want to...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [ate Stack ......................................   \n",
       "1  [WS account 1. Open https://portal.aws.amazon....   \n",
       "2  [ble. Note: The Regions listed in the picker a...   \n",
       "3  [WS account 1. Open https://portal.aws.amazon....   \n",
       "4  [perform: iam:PassRole In this case, Mary's po...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  This question is not related to AWS documentat...  \n",
       "1  This question is not related to AWS documentat...  \n",
       "2  This question is not related to AWS documentat...  \n",
       "3  To sign up for an AWS account open https://por...  \n",
       "4  You can create a role that users in other acco...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf2bbb",
   "metadata": {},
   "source": [
    "### Convert DataFrame to RAGAS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ad602e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_dataset = Dataset.from_pandas(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b476a073",
   "metadata": {},
   "source": [
    "# RAGAS Evaluation\n",
    "\n",
    "### LLM Evaluator Setup\n",
    "\n",
    "For evaluation, we use Claude Sonnet 4.5 because it provides higher quality responses compared to Haiku. We wrap it using RAGAS's LangChain wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b333a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(\n",
    "    ChatAnthropic(\n",
    "        model='claude-sonnet-4-5-20250929',\n",
    "        anthropic_api_key=api_key,\n",
    "        temperature=0,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7142c6",
   "metadata": {},
   "source": [
    "### Embedding Function Setup \n",
    "\n",
    "We use the same embedding function as in the RAG pipeline, wrapped for RAGAS compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edbed946",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = HuggingFaceEmbeddings(\n",
    "        model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "        model_kwargs={'device': device},\n",
    "        encode_kwargs={'normalize_embeddings': True, 'batch_size': 32},\n",
    "    )\n",
    "\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361429e6",
   "metadata": {},
   "source": [
    "### Run Evaluation\n",
    "\n",
    "We use RAGAS to compute four metrics: answer relevancy, faithfulness, context precision, and context recall. The batch size is set to 4 to reduce API call rate and avoid timeout errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d1a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_results = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=evaluator_embeddings,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "metrics_df = ragas_results.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327980a",
   "metadata": {},
   "source": [
    "Print results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132daa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average value for every metric')\n",
    "metrics_df[['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49cd711",
   "metadata": {},
   "source": [
    "# Chunking Strategy Evaluation\n",
    "\n",
    "### Define Test Configurations \n",
    "\n",
    "Using RAGAS, we can evaluate different chunking strategies, retrieval sizes ($k$), and embedding functions. In this project, we focus on chunking strategies only. We fix the embedding function and $k=5$ as a good trade-off between cost and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5672d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_configs = [\n",
    "    {'chunk_size': 200, 'overlap': 50, 'EOS':25, 'k': 5},\n",
    "    {'chunk_size': 500, 'overlap': 100, 'EOS':50, 'k': 5},\n",
    "    {'chunk_size': 1000, 'overlap': 200, 'EOS':100, 'k': 5},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98142e",
   "metadata": {},
   "source": [
    "### Load PDF Document \n",
    "\n",
    "We use the same AWS documentation as before. This approach can be applied to any technical document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032ebfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_URL = 'https://docs.aws.amazon.com/pdfs/vsts/latest/userguide/vsts-ug.pdf'\n",
    "PDF_NAME = 'aws_vsts.pdf'\n",
    "\n",
    "download_file(url=PDF_URL, name=PDF_NAME, overwrite=False)\n",
    "\n",
    "df_pdf = load_and_analyze_pdf(Path('../data/raw/') / PDF_NAME)\n",
    "Path('../data/processed/').mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9716088",
   "metadata": {},
   "source": [
    "## Run Evaluation Loop\n",
    "\n",
    "For each configuration:\n",
    "1. Create chunks with specified size\n",
    "2. Build vector store\n",
    "3. Run RAG pipeline on test dataset\n",
    "4. Calculate RAGAS metrics\n",
    "5. Store averaged results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb766942",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "for config in tqdm(chunk_configs, desc='Testing configurations'):\n",
    "    print(f\"\\nTesting: chunk_size = {config['chunk_size']}\")\n",
    "\n",
    "    # 1. Create chunks with specific size\n",
    "    chunks = create_chunks(\n",
    "        pages_data=df_pdf,\n",
    "        chunk_size=config['chunk_size'],\n",
    "        overlap=config['overlap'],\n",
    "        EOS=config['EOS'],\n",
    "    )\n",
    "    chunks.to_json(\n",
    "        path_or_buf=f'../data/processed/chunks_{config[\"chunk_size\"]}.json',\n",
    "        orient='records',\n",
    "        force_ascii=False,\n",
    "        indent=4\n",
    "    )\n",
    "\n",
    "    # 2. Create vectorstore\n",
    "    vectorstore = setup_data_collection(\n",
    "        chunks_filename=f'chunks_{config[\"chunk_size\"]}',\n",
    "        collection_name=f'aws_docs_langchain_{config[\"chunk_size\"]}',\n",
    "        overwrite=True,\n",
    "    )\n",
    "\n",
    "    # 3. Run throug RAG pipeline\n",
    "    eval_results = []\n",
    "    for idx, row in test_df.iterrows():\n",
    "        rag_result = langchain_rag_pipeline(\n",
    "            query=row['question'],\n",
    "            vectorstore=vectorstore,\n",
    "            k=config['k'],\n",
    "            llm_name='claude-haiku-4-5-20251001',\n",
    "        )\n",
    "        eval_results.append({\n",
    "            'question': row['question'],\n",
    "            'answer': rag_result['answer'],\n",
    "            'contexts': rag_result['contexts'],\n",
    "            'ground_truth': row['reference_answer'],\n",
    "        })\n",
    "\n",
    "    eval_df = pd.DataFrame(eval_results)\n",
    "\n",
    "    # Save and load dataframe\n",
    "    eval_df.to_pickle(f'../data/processed/eval_results_{config[\"chunk_size\"]}.pkl')\n",
    "    eval_df = pd.read_pickle(f'../data/processed/eval_results_{config[\"chunk_size\"]}.pkl')\n",
    "\n",
    "\n",
    "    # 4. Calculate RAGAS metrics\n",
    "    ragas_dataset = Dataset.from_pandas(eval_df)\n",
    "\n",
    "    ragas_results = evaluate(\n",
    "        ragas_dataset,\n",
    "        metrics=[\n",
    "            answer_relevancy,\n",
    "            faithfulness,\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "        ],\n",
    "        llm=evaluator_llm,\n",
    "        embeddings=evaluator_embeddings,\n",
    "        batch_size=4,\n",
    "    )\n",
    "\n",
    "    metrics_df = ragas_results.to_pandas()\n",
    "\n",
    "    # 5. Store results\n",
    "    all_results.append({\n",
    "        'chunk_size': config['chunk_size'],\n",
    "        'k': config['k'],\n",
    "        'answer_relevancy': metrics_df['answer_relevancy'].mean(),\n",
    "        'faithfulness': metrics_df['faithfulness'].mean(),\n",
    "        'context_precision': metrics_df['context_precision'].mean(),\n",
    "        'context_recall': metrics_df['context_recall'].mean(),\n",
    "    })\n",
    "\n",
    "\n",
    "comparison_df = pd.DataFrame(all_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8cd48",
   "metadata": {},
   "source": [
    "Save and load the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ce60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.to_pickle('../data/processed/comparison_df.pkl')\n",
    "\n",
    "comparison_df = pd.read_pickle('../data/processed/comparison_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121a0a4f",
   "metadata": {},
   "source": [
    "### Analyse Results\n",
    "Display evaluation results sorted by faithfulness score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17b107fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>k</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.690639</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.674306</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0.638778</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.652315</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.441749</td>\n",
       "      <td>0.943452</td>\n",
       "      <td>0.649630</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_size  k  answer_relevancy  faithfulness  context_precision  \\\n",
       "2        1000  5          0.690639      0.991667           0.674306   \n",
       "1         500  5          0.638778      0.968254           0.652315   \n",
       "0         200  5          0.441749      0.943452           0.649630   \n",
       "\n",
       "   context_recall  \n",
       "2        1.000000  \n",
       "1        0.933333  \n",
       "0        0.900000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df.sort_values(by='faithfulness', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29caeda6",
   "metadata": {},
   "source": [
    "**Observations:** \n",
    "- Larger chunk sizes consistently yield better scores across all metrics\n",
    "- All chunking strategies achieve **faithfulness** scores above 0.9\n",
    "- The most significant variation across configurations is in **answer_relevancy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1c3826",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this notebook the following steps were accomplished: \n",
    "\n",
    "1. Created evaluation dataset of 15 test questions (12 relevant, 3 irrelevant) with reference answers and page numbers\n",
    "\n",
    "2. Implemented RAGAS evaluation with 4 metrics:\n",
    "   - Answer Relevancy - how well answers address the question\n",
    "   - Faithfulness - whether answers stay grounded in context\n",
    "   - Context Precision - quality of retrieved chunks\n",
    "   - Context Recall - coverage of relevant information\n",
    "\n",
    "3. Tested 3 chunking strategies (200, 500, 1000 char chunks)\n",
    "\n",
    "**Key Findings:**\n",
    "- Larger chunks (1000 chars) perform best** across all metrics\n",
    "- High faithfulness (>0.9) - minimal hallucination across all configs\n",
    "- Answer relevancy improves significantly with larger chunks (0.44 â†’ 0.69)\n",
    "- Perfect context recall (1.0) with 1000-char chunks\n",
    "\n",
    "\n",
    "**Chunking configuration for the next step:**\n",
    "- Chunk size: 1000 characters\n",
    "- Overlap: 200 characters\n",
    "- k: 5 retrieved chunks\n",
    "- End of sentence: 100\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
